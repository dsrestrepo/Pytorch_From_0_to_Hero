{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28853cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 355.8032531738281\n",
      "199 331.9528503417969\n",
      "299 308.64984130859375\n",
      "399 285.9933776855469\n",
      "499 264.1024475097656\n",
      "599 243.1175537109375\n",
      "699 223.201171875\n",
      "799 204.53431701660156\n",
      "899 187.30654907226562\n",
      "999 171.69764709472656\n",
      "1099 157.8492431640625\n",
      "1199 145.8312225341797\n",
      "1299 135.6141357421875\n",
      "1399 127.06130981445312\n",
      "1499 119.9486312866211\n",
      "1599 114.00627136230469\n",
      "1699 108.96340942382812\n",
      "1799 104.58219146728516\n",
      "1899 100.67333984375\n",
      "1999 97.09760284423828\n",
      "2099 93.75861358642578\n",
      "2199 90.59331512451172\n",
      "2299 87.56261444091797\n",
      "2399 84.64397430419922\n",
      "2499 81.82567596435547\n",
      "2599 79.10304260253906\n",
      "2699 76.47573852539062\n",
      "2799 73.94607543945312\n",
      "2899 71.51778411865234\n",
      "2999 69.19527435302734\n",
      "3099 66.9831771850586\n",
      "3199 64.8858642578125\n",
      "3299 62.90715789794922\n",
      "3399 61.050113677978516\n",
      "3499 59.31679153442383\n",
      "3599 57.708065032958984\n",
      "3699 56.22353744506836\n",
      "3799 54.86146926879883\n",
      "3899 53.6187744140625\n",
      "3999 52.491119384765625\n",
      "4099 51.47301483154297\n",
      "4199 50.55807113647461\n",
      "4299 49.73918151855469\n",
      "4399 49.00875473022461\n",
      "4499 48.358978271484375\n",
      "4599 47.78203582763672\n",
      "4699 47.27027893066406\n",
      "4799 46.81637954711914\n",
      "4899 46.41347885131836\n",
      "4999 46.055206298828125\n",
      "5099 45.735774993896484\n",
      "5199 45.449974060058594\n",
      "5299 45.193138122558594\n",
      "5399 44.96116256713867\n",
      "5499 44.75043869018555\n",
      "5599 44.55781936645508\n",
      "5699 44.38060760498047\n",
      "5799 44.21643829345703\n",
      "5899 44.06332778930664\n",
      "5999 43.91954040527344\n",
      "6099 43.78361892700195\n",
      "6199 43.65431594848633\n",
      "6299 43.530582427978516\n",
      "6399 43.41154098510742\n",
      "6499 43.29640579223633\n",
      "6599 43.1845817565918\n",
      "6699 43.07550048828125\n",
      "6799 42.96873474121094\n",
      "6899 42.8639030456543\n",
      "6999 42.760684967041016\n",
      "7099 42.65882110595703\n",
      "7199 42.55807113647461\n",
      "7299 42.45826721191406\n",
      "7399 42.359249114990234\n",
      "7499 42.260902404785156\n",
      "7599 42.16307067871094\n",
      "7699 42.06571960449219\n",
      "7799 41.968746185302734\n",
      "7899 41.8720817565918\n",
      "7999 41.77568435668945\n",
      "8099 41.67951583862305\n",
      "8199 41.58353042602539\n",
      "8299 41.48768997192383\n",
      "8399 41.39197540283203\n",
      "8499 41.296382904052734\n",
      "8599 41.20088195800781\n",
      "8699 41.10543441772461\n",
      "8799 41.010093688964844\n",
      "8899 40.91474914550781\n",
      "8999 40.81950378417969\n",
      "9099 40.72425079345703\n",
      "9199 40.62905502319336\n",
      "9299 40.53390884399414\n",
      "9399 40.43873596191406\n",
      "9499 40.343624114990234\n",
      "9599 40.24855422973633\n",
      "9699 40.15342712402344\n",
      "9799 40.05834197998047\n",
      "9899 39.96330261230469\n",
      "9999 39.86830139160156\n",
      "10099 39.77322006225586\n",
      "10199 39.67816925048828\n",
      "10299 39.58314895629883\n",
      "10399 39.4881591796875\n",
      "10499 39.39319610595703\n",
      "10599 39.298213958740234\n",
      "10699 39.20318603515625\n",
      "10799 39.10818099975586\n",
      "10899 39.01320266723633\n",
      "10999 38.91824722290039\n",
      "11099 38.82331085205078\n",
      "11199 38.728397369384766\n",
      "11299 38.63350296020508\n",
      "11399 38.538543701171875\n",
      "11499 38.443572998046875\n",
      "11599 38.34862518310547\n",
      "11699 38.25368881225586\n",
      "11799 38.15877151489258\n",
      "11899 38.063873291015625\n",
      "11999 37.968994140625\n",
      "12099 37.87413024902344\n",
      "12199 37.7792854309082\n",
      "12299 37.6844596862793\n",
      "12399 37.589595794677734\n",
      "12499 37.494686126708984\n",
      "12599 37.39979553222656\n",
      "12699 37.3049201965332\n",
      "12799 37.210060119628906\n",
      "12899 37.115211486816406\n",
      "12999 37.020389556884766\n",
      "13099 36.92558288574219\n",
      "13199 36.83079147338867\n",
      "13299 36.73601150512695\n",
      "13399 36.64125442504883\n",
      "13499 36.546512603759766\n",
      "13599 36.45174026489258\n",
      "13699 36.35691452026367\n",
      "13799 36.26210403442383\n",
      "13899 36.16730880737305\n",
      "13999 36.07252883911133\n",
      "14099 35.97776794433594\n",
      "14199 35.88302230834961\n",
      "14299 35.78829574584961\n",
      "14399 35.693580627441406\n",
      "14499 35.5988883972168\n",
      "14599 35.504207611083984\n",
      "14699 35.4095458984375\n",
      "14799 35.314903259277344\n",
      "14899 35.22018051147461\n",
      "14999 35.125450134277344\n",
      "15099 35.030738830566406\n",
      "15199 34.9360466003418\n",
      "15299 34.841373443603516\n",
      "15399 34.7467155456543\n",
      "15499 34.65207290649414\n",
      "15599 34.55744934082031\n",
      "15699 34.46284484863281\n",
      "15799 34.36825942993164\n",
      "15899 34.273685455322266\n",
      "15999 34.17913055419922\n",
      "16099 34.08456802368164\n",
      "16199 33.98992919921875\n",
      "16299 33.89530563354492\n",
      "16399 33.80070114135742\n",
      "16499 33.70611572265625\n",
      "16599 33.611549377441406\n",
      "16699 33.516998291015625\n",
      "16799 33.42247009277344\n",
      "16899 33.32795333862305\n",
      "16999 33.233463287353516\n",
      "17099 33.13898468017578\n",
      "17199 33.04452896118164\n",
      "17299 32.95008850097656\n",
      "17399 32.85554504394531\n",
      "17499 32.76102828979492\n",
      "17599 32.66653060913086\n",
      "17699 32.572044372558594\n",
      "17799 32.47758865356445\n",
      "17899 32.38313293457031\n",
      "17999 32.28871154785156\n",
      "18099 32.19430160522461\n",
      "18199 32.099910736083984\n",
      "18299 32.00554275512695\n",
      "18399 31.911184310913086\n",
      "18499 31.81685447692871\n",
      "18599 31.722415924072266\n",
      "18699 31.62801170349121\n",
      "18799 31.533618927001953\n",
      "18899 31.43924903869629\n",
      "18999 31.344898223876953\n",
      "19099 31.250560760498047\n",
      "19199 31.15625762939453\n",
      "19299 31.06195640563965\n",
      "19399 30.967695236206055\n",
      "19499 30.873437881469727\n",
      "19599 30.77920913696289\n",
      "19699 30.684965133666992\n",
      "19799 30.590656280517578\n",
      "19899 30.496370315551758\n",
      "19999 30.402095794677734\n",
      " the gradient in last itteration are -20.01370620727539, -22.78921890258789, 3.45161771774292, 3.2416019439697266\n",
      "Result: y = -0.7044118642807007 + 0.18604879081249237 x + 0.1215219497680664 x^2 + 0.0020697133149951696 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(20000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    # MSE\n",
    "    loss = ((y_pred - y)**2).sum()\n",
    "    # RMSE\n",
    "    loss = ((y_pred - y)**2).sum().sqrt()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        if (t == 20000-1):\n",
    "            print(f' the gradient in last itteration are {a.grad}, {b.grad}, {c.grad}, {d.grad}')\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        \n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616dd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
